# CPS843_Comparative-Analysis-Between-YOLOv8-and-Faster-R-CNN
This study includes a literature review and  a quantitative analysis of two real time object detection algorithms. The YOLOv8 and Faster R-CNN algorithms were both tested using the same custom dataset of images to acquire results on accuracy and speed of each algorithm. 
Real-time object detection is a vitally important topic in computer vision, with applications in areas such as autonomous driving, security, and medicine [1]. Object detection is a process in computer vision in which objects in a photo or a video are located, identified, and labeled [2]. This task is commonly divided into two approaches: one-stage detection and two-stage detection. One-stage detectors require only one pass over the source image to identify objects within, while two-stage detectors require two passes. For this reason, one-shot detectors tend to be faster and two-stage detectors tend to have a higher degree of accuracy [2]. To study this difference,  a comparison will be made between a state of the art single-stage detector, “YOLOv8,” and a state of the art two-stage detector, “Faster R-CNN.”  By quantitatively comparing the time and accuracy results of these two algorithms, the aim of this study is to identify which approach has the better performance when detecting objects in real time.
Figure: Object Detector Anatomy [3]                                                                                                                                                     
II. Technical Overview
A. YOLOv8
You Only Look Once (YOLO), a groundbreaking real-time object detection system, was developed by Joseph Redmon and Ali Farhadi at the University of Washington in 2015. YOLO's revolutionary approach significantly influenced the field of computer vision and continued to evolve with subsequent versions.
YOLOv1 (2015): The initial version, YOLOv1, introduced the concept of processing images in a single pass through a convolutional network [3]. This approach enabled YOLO to achieve real-time object detection, processing images at 30 FPS on a Pascal Titan X. YOLOv1's key innovation was its ability to predict bounding boxes and class probabilities simultaneously.
Figure: YOLOv1 Preliminary Architecture [3]
YOLOv2 (2016): Building upon the success of YOLOv1, YOLOv2 (also known as YOLO9000) was released in 2016. This version incorporated improvements such as batch normalization, anchor boxes, and dimension clusters. These enhancements contributed to better accuracy and efficiency in object detection tasks.
YOLOv3 (2018): In 2018, YOLOv3 further advanced the model's capabilities. It introduced a more efficient backbone network, multiple anchors, and spatial pyramid pooling. YOLOv3's multi-scale predictions and the use of the darknet-53 network improved overall performance, although it faced challenges with medium and large object detections.
YOLOv4 (2020): The release of YOLOv4 in 2020 marked another leap in innovation. YOLOv4 introduced features like Mosaic data augmentation, an anchor-free detection head, and a new loss function. These enhancements aimed at pushing the boundaries of speed, accuracy, and adaptability in object detection systems.
YOLOv5 (2020): In the same year, YOLOv5 was introduced, showcasing improvements in performance and user-friendly features. YOLOv5 introduced hyperparameter optimization, integrated experiment tracking, and automatic export to popular formats. Its versatility made it suitable for various vision AI tasks beyond detection, including segmentation, pose estimation, tracking, and classification.
YOLOv6 (2022): In 2022, YOLOv6 was open-sourced by Meituan and found application in the company's autonomous delivery robots. This version continued the trend of refining and adapting YOLO for specific real-world applications.
YOLOv7 (2022): YOLOv7 added new tasks, such as pose estimation on the COCO keypoints dataset, expanding the model's capabilities and addressing diverse use cases.
YOLOv8 (Current State): As of the latest developments, YOLOv8, presented by Ultralytics, represents the cutting edge of YOLO models. It builds upon the success of previous versions, offering enhanced performance, flexibility, and efficiency. YOLOv8 offers a substantial boost in processing speed over the previous iterations, making it ideal for applications requiring swift analysis of images. Its improved accuracy in detecting small objects, facilitated by a dynamic head network, enhances overall precision. The adoption of an anchor-free architecture simplifies training on diverse datasets, providing increased flexibility. 
YOLOv8's multi-scale prediction capabilities contribute to superior object detection across various sizes, addressing limitations observed in prior versions. With a more advanced backbone network, YOLOv8 excels in challenging environments, ensuring robust performance. These collective improvements make YOLOv8 a state-of-the-art choice, striking a balance between speed and accuracy for a wide range of vision AI tasks, including detection, segmentation, pose estimation, tracking, and classification.

Figure: The evolution of the YOLO neural networks family from v1 to v8 
YOLOv8 architecture builds upon its predecessors, leveraging a modified CSPDarknet53 backbone with 53 convolutional layers and cross-stage partial connections for enhanced information flow. The head incorporates convolutional and fully connected layers to predict bounding boxes, objectness scores, and class probabilities.
An anchor-free split Ultralytics head improves detection accuracy and efficiency compared to anchor-based methods. The neck combines Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) for creating and integrating feature maps of different object sizes, boosting overall accuracy. YOLOv8 supports model scaling with pre-trained sizes ranging from nano to extra-large, utilizing SiLU as the primary activation function.
For classification tasks, the architecture omits SPPF blocks and a neck, directly connecting the backbone to the classifier head. This design optimization achieves a balance between accuracy and speed, making YOLOv8 well-suited for real-time object detection across diverse applications.

Figure: YOLOv8 Architecture [6] 
B. Faster R-CNN
Faster R-CNN, short for "Faster Region-Convolutional Neural Network," is another state-of-the-art object detection algorithm.. It is part of the R-CNN family and aims to detect and locate objects in images with improved speed and accuracy using a  2-stage detection.
Similar to YOLOv8’s continuous improvement over time , the evolution of Faster R-CNN also stemmed from the predecessor, which was R-CNN, introduced by Ross Girshick and team. R-CNN utilized selective search for region proposal extraction, but it faced challenges such as prolonged training times and dependence on a fixed selective search algorithm, limiting its adaptability and proposal generation efficiency. 

Figure: R-CNN Architecture
In response to the limitations of R-CNN, Fast R-CNN emerged as a solution to accelerate object detection. By directly processing the input image through a Convolutional Neural Network (CNN) and introducing Region of Interest (RoI) pooling, Fast R-CNN eliminated the need for individual region proposal processing. This significantly improved processing speed during both training and testing. However, the reliance on regional proposals still impacted its overall performance. 
This paved the way for the subsequent development of Faster R-CNN to further enhance the efficiency and adaptability of object detection algorithms.

Figure: Fast R-CNN Architecture
The Faster R-CNN architecture comprises a Convolutional Neural Network (CNN) Backbone responsible for extracting essential features from input images, and these features are shared between the Region Proposal Network (RPN) and the Fast R-CNN detector. The RPN, tasked with generating region proposals, utilizes anchor boxes with varying scales and aspect ratios, employing a sliding window approach over feature maps obtained from the CNN backbone. It predicts objectness scores and bounding box adjustments, employing Intersection over Union (IoU) and Non-Maximum Suppression (NMS) for precise proposal selection. The Fast R-CNN Detector then takes these region proposals, undergoes Region of Interest (RoI) pooling, utilizes the CNN backbone for feature extraction, and employs fully connected layers for object classification and bounding box regression. A multi-task loss function combines classification and regression losses, followed by post-processing with Non-Maximum Suppression (NMS) to yield refined detection results.

Figure: Faster R-CNN is a single, unified network for object detection. The RPN module serves as the ‘attention’ of this unified network. [8]

